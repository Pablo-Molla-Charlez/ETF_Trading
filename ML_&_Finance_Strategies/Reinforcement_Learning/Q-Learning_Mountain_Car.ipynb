{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mountain Car - Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description of the Problem\n",
    "\n",
    "- In this Jupyter notebook, we explore the **Mountain Car Markov Decision Process** (MDP), a classic example in reinforcement learning. \n",
    "\n",
    "- This scenario features a car situated randomly at the lowest point of a sinusoidal valley. The challenge lies in applying strategic accelerations to propel the car in either direction, aiming to reach the summit of the hill on the right. \n",
    "\n",
    "- The environment presents two distinct variations within the gym framework: one that operates on discrete action spaces and another on continuous action spaces. Our focus will be on the discrete action version, where the objective is to master the art of maneuvering the car to the goal state by making precise and calculated decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](mountain_car.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the context of the Mountain Car problem, as implemented in the Gym library by OpenAI, let's understand a simplified breakdown of the key components:\n",
    "\n",
    "### 1. $\\fbox{Agent}$\n",
    "The agent in this environment is the car itself. The agent's goal is to learn how to reach the top of the right hill/mountain. The challenge is that the car doesn't initially have enough power to simply accelerate up the hill; it must learn to leverage momentum by going back and forth between the hills.\n",
    "\n",
    "### 2. $\\fbox{Possible Actions}$\n",
    "In the discrete action space version of the Mountain Car environment, there are three possible actions the agent can take at each step:\n",
    "\n",
    "1. Accelerate to the left\n",
    "2. Do nothing\n",
    "3. Accelerate to the right\n",
    "\n",
    "These actions affect the car's velocity and position on the hill.\n",
    "\n",
    "### 3. $\\fbox{State}$\n",
    "A state in this environment is represented by a 2-dimensional observation:\n",
    "- The car's current position (ranging from -1.2 to 0.6, where -1.2 is the far left of the environment and 0.6 is the far right)\n",
    "- The car's current velocity (which can also be negative or positive depending on the direction of movement but always within [-0.07, 0.07])\n",
    "\n",
    "This observation space is defined as a `Box` in Gym, which means each component of the state (position and velocity) can take any value within a continuous range.\n",
    "\n",
    "### 4. $\\fbox{Reward}$\n",
    "The reward structure is straightforward: the agent receives a reward of -1 for every time step that the car does not reach the goal. This means the agent is incentivized to reach the goal as quickly as possible since the longer it takes, the more negative the total reward becomes.\n",
    "\n",
    "### 5. $\\fbox{How it Works}$\n",
    "The agent (car) interacts with the environment (the valley) by taking actions at each time step. After each action, the environment responds by updating the agent's state (its position and velocity) and providing a reward signal. The agent's objective is to learn a policy—a strategy for choosing actions based on the current state—that maximizes the total reward over time. In the Mountain Car problem, this involves learning to build momentum by swinging back and forth until the car has enough energy to reach the goal on top of the right hill."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](gym.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI's Library GYM\n",
    "\n",
    "1. $\\fbox{Introduction}$\n",
    "\n",
    "- The library **gym** implements the Mountain Car MDP along with a wide variety of other environments designed to develop and test reinforcement learning algorithms. \n",
    "\n",
    "- Gym is an open-source library created by OpenAI, designed to provide a simple and universal interface to a collection of environments with different complexities, ranging from simple toy problems to challenging, real-world scenarios.\n",
    "\n",
    "\n",
    "2. $\\fbox{Key Features of Gym:}$\n",
    "\n",
    "- Standardized Environments: Gym offers a collection of environments with a standardized interface, making it easier for developers to test algorithms on different problems without needing to adjust to new APIs.\n",
    "\n",
    "- Diverse Challenges: The library includes a diverse set of environments, from classic control tasks like the CartPole and Mountain Car to Atari video games and robotic simulations, catering to various levels of reinforcement learning challenges.\n",
    "\n",
    "- Easy to Use: Gym is designed to be user-friendly, allowing researchers and enthusiasts to get started with reinforcement learning quickly. With just a few lines of code, one can set up an environment, interact with it, and begin experimenting with different reinforcement learning strategies.\n",
    "\n",
    "- Extensible: While Gym provides a solid base of environments, it also supports the creation and addition of custom environments. This flexibility encourages the development and testing of algorithms on novel tasks.\n",
    "\n",
    "- Benchmarking: Gym environments are used as benchmarks in the reinforcement learning community, providing a common ground for comparing the performance of algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym==0.17.3 in c:\\users\\pmoll\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.17.3)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: scipy in c:\\users\\pmoll\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gym==0.17.3) (1.11.2)\n",
      "Requirement already satisfied: numpy>=1.10.4 in c:\\users\\pmoll\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gym==0.17.3) (1.26.3)\n",
      "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in c:\\users\\pmoll\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gym==0.17.3) (1.5.0)\n",
      "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in c:\\users\\pmoll\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gym==0.17.3) (1.6.0)\n",
      "Requirement already satisfied: future in c:\\users\\pmoll\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pyglet<=1.5.0,>=1.4.0->gym==0.17.3) (0.18.3)\n"
     ]
    }
   ],
   "source": [
    "# I had problems executing the last version because it didn't appear the window with the moving car \n",
    "%pip install gym==0.17.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from random import randint\n",
    "\n",
    "# Create the MountainCar environment\n",
    "environment = gym.make(\"MountainCar-v0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Possible actions for the agent (car) :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 0 : Accelerates to the left\n",
    "\n",
    "- 1 : Does nothing\n",
    "\n",
    "- 2 : Accelerates to the right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(3)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Discrete actions for the car's mouvement\n",
    "environment.action_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observation Space: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- An $\\textcolor{green}{\\text{observation space}}$ (observation_space): Defines the format of observations that the agent receives from the environment to understand its current state. Observations can be as simple as positions and velocities or as complex as pixel data from a camera.\n",
    "\n",
    "- The observation space is represented as a Box, which is a way to define a space of valid observations in the gym environment. The Box space represents an n-dimensional box, meaning the observations will be real-valued vectors of a specified range.\n",
    "\n",
    "- The range of each element in the observation vector is between -1.2000000476837158 and 0.6000000238418579. This range specifies the minimum and maximum values for the observations the agent can receive. In this case, the $\\textcolor{red}{\\text{position of the car}}$ is settled in the range: [-1.2, 0.6]\n",
    "\n",
    "- This means that the maximum position in which the car can be at the left side (top left of the mountain - opposite position to the flag) is -1,2 and the maximum at the right side (top right of the mountain - at at the flag). On the other hand, the $\\textcolor{red}{\\text{velocity}}$ is clipped to the range [-0,07, 0,07] meaning that it can reach the same velocity on both directions.\n",
    "\n",
    "- The (2,) indicates the shape of the observation vector, meaning each observation consists of 2 elements. In the context of the Mountain Car environment, these two elements typically represent the $\\textcolor{blue}{\\text{position}}$ and $\\textcolor{blue}{\\text{velocity}}$ of the car.\n",
    "\n",
    "- float32 specifies the data type of the observations, indicating that each element in the observation vector is a 32-bit floating-point number.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Range position of the agent (car)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(-1.2000000476837158, 0.6000000238418579, (2,), float32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "environment.observation_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting position of the agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The position of the car is assigned a uniform random value in [-0.6 , -0.4]. The starting velocity of the car is always assigned to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.53249877,  0.        ])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To access starting position\n",
    "environment.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Termination of Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The experiement ends in 2 possible situations:\n",
    "\n",
    "- Truncation: The number of actions taken (nº of episodes) is 200 or if the car surpasses 200 actions.\n",
    "\n",
    "- Termination: The agent's position is greater or equal than 0.5.\n",
    "\n",
    "In first case it didn't reach the goal within the corresponding nº of episodes and in the latter the goal was achieved.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step Action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **step** function is a critical component that simulates the agent's interaction with the environment. Here’s a breakdown of how it works and what it produces:\n",
    "\n",
    "### The step(action) Function\n",
    "\n",
    "When you call **step(action)** on an environment object, you're asking the environment to simulate one timestep of the agent taking a specified action. This action is an input to the function, representing the agent's decision based on its current policy or strategy.\n",
    "\n",
    "### What step(action) Returns\n",
    "\n",
    "The **step** function returns four key pieces of information as a tuple: **(new_state, reward, done, info)**.\n",
    "\n",
    "1. **new_state**: This is the new state of the environment after the agent's action has been applied. It reflects the consequence of the action and is what the agent will observe next before making its subsequent decision. The state can include various pieces of information depending on the environment, such as the position and velocity of an object in a physical simulation.\n",
    "\n",
    "2. **reward**: This is a numerical value that the environment returns to the agent as feedback for the action it took. The reward is a critical component of reinforcement learning, as it guides the agent's learning process. The agent's goal is typically to maximize the cumulative reward it receives over time.\n",
    "\n",
    "3. **done**: This is a boolean flag indicating whether the episode has ended. An episode might end because the agent has achieved the goal, failed in some way, or simply because a maximum number of timesteps has been reached. When done is True, the agent should reset the environment to start a new episode.\n",
    "\n",
    "4. **info**: This is a dictionary that can provide additional information about the timestep, which might be useful for debugging or for learning algorithms that require more context. The contents of info are specific to each environment and typically not used for training the agent.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Equations behind the Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $\\fbox{\\text{velocity\\_{t+1} = velocity\\_{t} + (action - 1) * force - cos(3 * positiont) * gravity}} $ where force = 0.001 and gravity = 0.0025. \n",
    "\n",
    "- $\\fbox{\\text{position\\_{t+1} = position\\_{t} + velocity\\_{t+1}}}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mountain Car In-Context\n",
    "\n",
    "Consider an environment where an agent controls a car trying to reach the top of a hill, such as the Mountain Car environment. The agent can choose actions like accelerating to the left, right, or not accelerating at all. Each action affects the car's velocity and position:\n",
    "\n",
    "- **Action**: Accelerate to the right.\n",
    "- **new_state**: The car's new position and velocity after acceleration.\n",
    "- **reward**: Often, a small negative value to encourage reaching the goal with fewer steps.\n",
    "- **done**: Whether the car has reached the goal position or the maximum number of timesteps.\n",
    "- **inf**: Might include details like the car's maximum velocity during the step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "environment.reset()\n",
    "final = False\n",
    "while not final:\n",
    "    # go to the left side\n",
    "    action = 1\n",
    "    # to apply the action to the car we use .step(action) which returns 4 values:\n",
    "    # the new_state, the reward, if it reached or not the final position and some info we don't need\n",
    "    new_state, reward, final, info = environment.step(action)\n",
    "    # after 200 episodes the library converts final to true automatically to stop it\n",
    "    #print(final)\n",
    "    # to display the car's movement\n",
    "    environment.render()\n",
    "environment.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random movement - Non sense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "environment.reset()\n",
    "final = False\n",
    "while not final:\n",
    "    # random action\n",
    "    action = randint(0,2)\n",
    "    # to apply the action to the car we use .step(action) which returns 4 values:\n",
    "    # the new_state, the reward, if it reached or not the final position and some info we don't need\n",
    "    new_state, reward, final, info = environment.step(action)\n",
    "    #print(final)\n",
    "    # to display the car's movement\n",
    "    environment.render()\n",
    "environment.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Table Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](q_table.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A Q-table is a fundamental component in the field of reinforcement learning, specifically in Q-learning, a type of model-free reinforcement learning algorithm. The \"Q\" in Q-learning stands for quality, and the Q-table represents the quality of a certain action taken in a given state. \n",
    "\n",
    "- Essentially, it is a lookup table where each row corresponds to a state and each column corresponds to the possible actions the agent can take. The values stored in the table, known as Q-values, quantify the expected utility of taking an action in a particular state, considering the long-term return.\n",
    "\n",
    "- The Q-table helps the agent to choose the best action to perform in a given state by consulting the table for the action with the highest Q-value. Over time, through a process of exploration (trying out new actions) and exploitation (using known information to make decisions), the Q-values in the table are updated based on the rewards received from the environment. This process continues until the Q-values converge, meaning the agent learns the optimal action to take in each state to maximize its cumulative reward.\n",
    "\n",
    "### Structure of a Q-table:\n",
    "\n",
    "- **Rows**: Represent the different states in which the agent can be.\n",
    "- **Columns**: Represent the possible actions the agent can take in those states.\n",
    "- **Values (Q-values)**: Each cell in the table contains a Q-value, which is an estimation of the total reward an agent can expect to receive by taking a given action from a given state, following a certain policy thereafter.\n",
    "\n",
    "### Updating the Q-table:\n",
    "\n",
    "The Q-values are updated using the Bellman equation, which incorporates the immediate reward received from taking an action, plus the discounted maximum future reward expected from the next state. This can be mathematically represented as:\n",
    "\n",
    "$$ Q(s, a) = Q(s, a) + \\alpha [r + \\gamma \\max_{a'}Q(s', a') - Q(s, a)] $$\n",
    "\n",
    "where:\n",
    "- Q(s, a) is the current Q-value of state s and action a,\n",
    "- $\\alpha$ is the learning rate,\n",
    "- r is the immediate reward,\n",
    "- $\\gamma$ is the discount factor determining the importance of future rewards,\n",
    "- $\\max_{a'}Q(s', a')$ is the estimate of the optimal future value.\n",
    "\n",
    "Through repeated interaction with the environment and updating of the Q-values in the table, the agent learns the best action to take in each state to maximize its rewards, effectively solving the reinforcement learning problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discretization of the agent's position"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **.reset()** method returns the initial state of the environment, which, for the Mountain Car, consists of two continuous values: the car's position and velocity.\n",
    "Let's understand how these values could be discretized:\n",
    "\n",
    "### Understanding the Initial State\n",
    "\n",
    "When you call **.reset()** in the Mountain Car environment, it returns an initial state like $[-0.5, 0.0]$, where the first value represents the position of the car (between -1.2 and 0.6) and the second value represents the velocity of the car (between -0.07 and 0.07).\n",
    "\n",
    "### Discretization Strategy\n",
    "\n",
    "To discretize this state, you divide the range of possible values for each state variable (position and velocity) into a fixed number of bins or intervals. For example, you might choose to discretize the position into 20 bins and the velocity into 10 bins.\n",
    "\n",
    "### Example: Discretizing the Initial State\n",
    "\n",
    "Suppose the initial state returned by **.reset()** is $[-0.5, 0.0]$. Here's a step-by-step approach to discretize this state:\n",
    "\n",
    "1. **Define the bins for each state variable**:\n",
    "\n",
    "    - For position (-1.2 to 0.6), you might create 20 equally spaced bins.\n",
    "    \n",
    "    - For velocity (-0.07 to 0.07), you might create 10 equally spaced bins.\n",
    "\n",
    "2. **Calculate the bin width for each state variable**:\n",
    "\n",
    "    - Position bin width = (0.6 - (-1.2)) / 20 = 0.09\n",
    "    \n",
    "    - Velocity bin width = (0.07 - (-0.07)) / 10 = 0.014\n",
    "\n",
    "3. **Determine the bin index for each state variable**:\n",
    "\n",
    "    For the position (-0.5), calculate how many bin widths it is from the minimum value (-1.2):\n",
    "    \n",
    "    - Position index = (current_position - min_position) / position_bin_width = (-0.5 - (-1.2)) / 0.09 ≈ 7.77\n",
    "    \n",
    "    Since you can't have a fraction of a bin, you round this down to the nearest whole number, giving you $\\textcolor{green}{\\text{bin index 7}}$ for the position.\n",
    "    \n",
    "    For the velocity (0.0), you do the same:\n",
    "    \n",
    "    - Velocity index = (current_velocity - min_velocity) / velocity_bin_width = (0.0 - (-0.07)) / 0.014 ≈ 5\n",
    "    \n",
    "    This gives you $\\textcolor{green}{\\text{bin index 5}}$ for the velocity.\n",
    "\n",
    "4. **Resulting Discretized State**:\n",
    "\n",
    "    Your discretized state would be represented by the bin indexes $[7, 5]$, corresponding to the original continuous state $[-0.5, 0.0]$.\n",
    "\n",
    "This process allows you to convert continuous state variables into discrete ones, making it easier to work with tabular methods like Q-learning. It's important to choose the number of bins wisely, as too many bins can make learning slow and too few can oversimplify the state representation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discretization Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 10)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from math import *\n",
    "\n",
    "def discretize(value, number_bin_position, number_bin_velocity):\n",
    "    width_bin_position = (environment.observation_space.high[0] - environment.observation_space.low[0]) / number_bin_position\n",
    "    width_bin_velocity = (environment.observation_space.high[1] - environment.observation_space.low[1]) / number_bin_velocity\n",
    "    discretized_position = (value[0] - environment.observation_space.low[0]) / width_bin_position\n",
    "    discretized_velocity = (value[1] - environment.observation_space.low[1]) / width_bin_velocity\n",
    "    return (floor(discretized_position), floor(discretized_velocity))\n",
    "\n",
    "discretize(environment.reset(), 20, 20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numpy Q-Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.49175826,  0.90617779,  0.09905332],\n",
       "        [-0.49294974, -0.30529176,  0.55772132],\n",
       "        [ 0.17409564, -0.4592696 ,  0.52563634],\n",
       "        ...,\n",
       "        [ 0.24399346, -0.14551161,  0.57450157],\n",
       "        [-0.00203794, -0.53904631,  0.27765144],\n",
       "        [-0.97336561,  0.46415499,  0.62597338]],\n",
       "\n",
       "       [[-0.80302629, -0.85115562, -0.75226039],\n",
       "        [ 0.20693504,  0.08657028,  0.44539756],\n",
       "        [ 0.94982456,  0.31544208,  0.17838091],\n",
       "        ...,\n",
       "        [-0.98992282, -0.41494189, -0.99009513],\n",
       "        [-0.9351455 , -0.87907356, -0.71650754],\n",
       "        [ 0.38185928,  0.12433364, -0.2434085 ]],\n",
       "\n",
       "       [[-0.12894849, -0.06028009, -0.06217573],\n",
       "        [ 0.26024202, -0.34144747,  0.94579716],\n",
       "        [-0.58658125, -0.34356671,  0.11770746],\n",
       "        ...,\n",
       "        [-0.71564018, -0.79330394,  0.99529271],\n",
       "        [ 0.12651873, -0.99065724, -0.98618259],\n",
       "        [ 0.3989102 ,  0.24542916, -0.78771584]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 0.50454134, -0.9775264 , -0.98427679],\n",
       "        [-0.55007677, -0.92763644,  0.97432983],\n",
       "        [ 0.03569473,  0.74211379,  0.37935769],\n",
       "        ...,\n",
       "        [ 0.79050669, -0.43374922, -0.22958301],\n",
       "        [-0.04117234,  0.67508682, -0.90204672],\n",
       "        [ 0.82348654, -0.42522865,  0.92294914]],\n",
       "\n",
       "       [[ 0.36244449, -0.53355639, -0.13720962],\n",
       "        [-0.36174179,  0.89483   ,  0.60388723],\n",
       "        [ 0.30370071,  0.20718776,  0.7888237 ],\n",
       "        ...,\n",
       "        [ 0.28256147,  0.1116555 , -0.37230155],\n",
       "        [-0.94668616, -0.18024109,  0.15216759],\n",
       "        [-0.60658778,  0.74145986,  0.90913506]],\n",
       "\n",
       "       [[-0.89335307, -0.95399078, -0.09970123],\n",
       "        [-0.5382951 , -0.36025717, -0.0191029 ],\n",
       "        [ 0.51298114, -0.20673602,  0.63117762],\n",
       "        ...,\n",
       "        [-0.62747129,  0.76261783, -0.74499165],\n",
       "        [-0.66478691,  0.20565235,  0.20830536],\n",
       "        [-0.64170287, -0.97076936,  0.11417712]]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "q_table = np.random.uniform(low=-1, high=1, size=[20,20, 3])\n",
    "q_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation - Final Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Revisiting the fundamental algorithm, it's essential to incorporate the Q-Learning equation to enable the car to adapt based on its actions. We set a total of 5000 episodes, which are cycles where the agent executes an action and adjusts its strategy based on the received reward. \n",
    "\n",
    "- To avoid constant rendering, which would significantly slow down the process, we opt to render only once every 500 episodes. This approach ensures the simulation can complete 5000 iterations without excessive delay. \n",
    "\n",
    "- The discount rate, a value ranging from 0 to 1, plays a crucial role in shaping the agent's strategy: values closer to 1 encourage the pursuit of long-term rewards, while values nearer to 0 favor immediate gratification. In selecting the most prudent action, the one offering the highest reward in the current scenario, we also consider the discount rate's influence. A rate approaching 1 helps balance this selection, aligning the agent's decisions with a strategy that weighs future benefits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 100 - Reward: -200.0\n",
      "Episode: 200 - Reward: -200.0\n",
      "Episode: 300 - Reward: -200.0\n",
      "Episode: 400 - Reward: -200.0\n",
      "Episode: 500 - Reward: -200.0\n",
      "Episode: 600 - Reward: -200.0\n",
      "Episode: 700 - Reward: -200.0\n",
      "Episode: 800 - Reward: -200.0\n",
      "Episode: 900 - Reward: -200.0\n",
      "Episode: 1000 - Reward: -200.0\n",
      "Episode: 1100 - Reward: -200.0\n",
      "Episode: 1200 - Reward: -200.0\n",
      "Episode: 1300 - Reward: -200.0\n",
      "Episode: 1400 - Reward: -199.985\n",
      "Episode: 1500 - Reward: -199.986\n",
      "Episode: 1600 - Reward: -199.986875\n",
      "Episode: 1700 - Reward: -199.97823529411764\n",
      "Episode: 1800 - Reward: -199.97944444444445\n",
      "Episode: 1900 - Reward: -199.97210526315789\n",
      "Episode: 2000 - Reward: -199.909\n",
      "Episode: 2100 - Reward: -199.89238095238096\n",
      "Episode: 2200 - Reward: -199.89727272727274\n",
      "Episode: 2300 - Reward: -199.90173913043478\n",
      "Episode: 2400 - Reward: -199.88875\n",
      "Episode: 2500 - Reward: -199.8932\n",
      "Episode: 2600 - Reward: -199.8973076923077\n",
      "Episode: 2700 - Reward: -199.9011111111111\n",
      "Episode: 2800 - Reward: -199.88464285714286\n",
      "Episode: 2900 - Reward: -199.75551724137932\n",
      "Episode: 3000 - Reward: -199.76366666666667\n",
      "Episode: 3100 - Reward: -199.76258064516128\n",
      "Episode: 3200 - Reward: -199.750625\n",
      "Episode: 3300 - Reward: -199.4257575757576\n",
      "Episode: 3400 - Reward: -199.39617647058824\n",
      "Episode: 3500 - Reward: -199.41342857142857\n",
      "Episode: 3600 - Reward: -199.4297222222222\n",
      "Episode: 3700 - Reward: -199.38918918918918\n",
      "Episode: 3800 - Reward: -199.17184210526315\n",
      "Episode: 3900 - Reward: -198.8905128205128\n",
      "Episode: 4000 - Reward: -198.846\n",
      "Episode: 4100 - Reward: -198.81073170731707\n",
      "Episode: 4200 - Reward: -198.77357142857142\n",
      "Episode: 4300 - Reward: -198.7853488372093\n",
      "Episode: 4400 - Reward: -198.80227272727274\n",
      "Episode: 4500 - Reward: -198.642\n",
      "Episode: 4600 - Reward: -198.59478260869565\n",
      "Episode: 4700 - Reward: -198.60787234042553\n",
      "Episode: 4800 - Reward: -198.58458333333334\n",
      "Episode: 4900 - Reward: -198.59244897959184\n",
      "Episode: 5000 - Reward: -198.4244\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.02\n",
    "discount_rate = 0.95\n",
    "episodes = 5001\n",
    "reward_list = []\n",
    "\n",
    "for episode in range(1, episodes):\n",
    "    state = discretize(environment.reset(), 20, 20)\n",
    "    final = False\n",
    "    reward_accumulation = 0\n",
    "    while not final: # this while loop iterates 200 times\n",
    "        # wise action, we take the most rewarded action within the current state\n",
    "        action = np.argmax(q_table[state])\n",
    "        # to apply the action to the car we use .step(action) which returns 4 values:\n",
    "        # the new_state, the reward, if it reached or not the final position and some info we don't need\n",
    "        new_state, reward, final, info = environment.step(action)\n",
    "        q_table[state][action] = q_table[state][action] + learning_rate*(reward + discount_rate*np.max(q_table[discretize(new_state, 20, 20)]) - q_table[state][action])\n",
    "        state = discretize(new_state, 20, 20)\n",
    "        reward_accumulation = reward_accumulation + reward\n",
    "        if episode % 500 == 0:\n",
    "            # to display the car's movement\n",
    "            environment.render()\n",
    "    reward_list.append(reward_accumulation)\n",
    "    if episode % 100 == 0:\n",
    "        print(f\"Episode: {episode} - Reward: {np.mean(reward_list)}\")\n",
    "    environment.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hide & Seek - Reinforcement Learning : https://www.youtube.com/watch?v=kopoLzvh5jY&t=174s&ab_channel=OpenAI"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
